{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Definition (Atari Assault)\n",
    "\n",
    "The agent has to decide to choose an action in a space of 7 discrete space, which gives 18 possible actions. The observation state is RGB images of shape = (250, 160, 3) with integer values between [0-255]. \n",
    "\n",
    "You can learn more about it here:\n",
    "https://gymnasium.farama.org/environments/atari/assault/\n",
    "\n",
    "In this Notebook, we are going to experiment with two version of Q learning: (A) Deep Q Network and (B) PPO. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#This is the environment or game to be used with rgb output (from render)\n",
    "env = gym.make(\"ALE/Assault-v5\", render_mode='rgb_array')\n",
    "#env = gym.vector.make(\"ALE/Assault-v5\", render_mode='rgb_array', num_envs=3)\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "total_n_episodes = 1000\n",
    "\n",
    "max_steps = 300\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track episode duration when training / evaluating \n",
    "#Note: The total reward in this env represent the duration of an episode, which sometimes can be different.\n",
    "episode_rewards = []\n",
    "\n",
    "def plot_rewards(show_result=False):\n",
    "    plt.figure(1)\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration / Total Reward')\n",
    "    plt.plot(rewards_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(rewards_t) >= 100:\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    #The state \n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=256, kernel_size=3, stride=3)\n",
    "        self.cnn2 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=3)\n",
    "        self.pool = nn.MaxPool2d(3, stride=2)\n",
    "        self.cnn3 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=3, stride=3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(2048, 256)\n",
    "        self.fc2 = nn.Linear(256, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.cnn1(x))\n",
    "        x = F.relu(self.cnn2(x))\n",
    "        x = F.relu(self.cnn3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the frames by switching axes and normalizing values to floats\n",
    "#Input frames should be passed in a batch mode (batch, 3, h, w)\n",
    "def prepare_frame(state):\n",
    "    #Create a float tensor\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    #Permute shape from (batch, h, w, 3) to (batch, 3, h, w)\n",
    "    state = torch.permute(state, (0, 3, 1, 2))\n",
    "    #Reshape\n",
    "    state = torchvision.transforms.Resize(size=(112, 112))(state)\n",
    "    #Normalize values to obtain floats\n",
    "    state = state / 255.\n",
    "\n",
    "    return state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "LR = 1e-3\n",
    "\n",
    "#Track the duration of the epipsodes\n",
    "episode_rewards = []\n",
    "\n",
    "policy_net = DQN(n_actions).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state).to(device)\n",
    "    action_batch = torch.cat(batch.action).to(device)\n",
    "    reward_batch = torch.cat(batch.reward).to(device)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = policy_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to render and display the environment\n",
    "def display_environment(env, policy_net):\n",
    "    state, _ = env.reset()\n",
    "    plt.figure()\n",
    "\n",
    "    img = plt.imshow(env.render())  # Initialize the rendering with an empty frame\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    for t in range(1000):  # Run for a set number of time steps\n",
    "\n",
    "        state = prepare_frame(state).to(device)\n",
    "        action = policy_net(state).argmax().item()  # Select action\n",
    "        state, reward, terminated, truncated, _ = env.step(action)  # Take a step\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        \n",
    "        frame = env.render()  # Render the environment to an RGB array\n",
    "        img.set_data(frame)  # Update the rendering\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(frame)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        time.sleep(0.01)  # Pause to slow down the rendering\n",
    "\n",
    "        if done:\n",
    "            print(\"Game over..Your agent sucks!\")\n",
    "            break\n",
    "    \n",
    "    if not done:\n",
    "        print(\"Game over..Your agent is great!\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m     35\u001b[0m \u001b[39m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m optimize_model()\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m(done):\n\u001b[1;32m     39\u001b[0m     episode_rewards\u001b[39m.\u001b[39mappend(total_reward \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[49], line 28\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m next_state_values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(BATCH_SIZE, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     27\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 28\u001b[0m     next_state_values[non_final_mask] \u001b[39m=\u001b[39m policy_net(non_final_next_states)\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     29\u001b[0m \u001b[39m# Compute the expected Q values\u001b[39;00m\n\u001b[1;32m     30\u001b[0m expected_state_action_values \u001b[39m=\u001b[39m (next_state_values \u001b[39m*\u001b[39m GAMMA) \u001b[39m+\u001b[39m reward_batch\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 1000\n",
    "else:\n",
    "    num_episodes = 100\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = prepare_frame(state)\n",
    "    total_reward = 0\n",
    "\n",
    "    #first episodes are too long because NPCs take a long time to kill the player\n",
    "    for t in count():\n",
    "    #for t in range(max_steps):\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = prepare_frame(observation)\n",
    "            \n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        if(done):\n",
    "            episode_rewards.append(total_reward + 1)\n",
    "            plot_rewards()\n",
    "            break\n",
    "\n",
    "    \n",
    "\n",
    "print('Complete')\n",
    "plot_rewards(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAAGFCAYAAACorKVtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAi+ElEQVR4nO3deXCk5WHn8e/7vn1Kat33SBrNfQIBDBgbTOzY+MKQhHUOmySOgx17U1vJVqqy2V3vld04cWWTVDbrOEvsHGY3ibFTTgjgYLCNwTCcwzAzzKGR5tR9tfo+3u733T960IxQDwhmNNIz+n2qqEJ63rf1tAp9ed63337b8n3fR0TEEPZKT0BE5K1QtETEKIqWiBhF0RIRoyhaImIURUtEjKJoiYhRFC0RMUpgqRtalrWc8xARYSnXumulJSJGUbRExChLPjyU1eX6rY3s7o9VHXtlKMm+oUTVsZ3rY9ywrbHq2NEzaZ49HK86trGrhluvaqk6dmoixxOvTFcd626J8BPXtWFXObswES/w3Rcn8c47IggFLD58YweNdcGqj3e+42NZnjowA4Btwe3vaKejKfym+43O5Hl87xR6162ZFC0DWRbctL2Jn3vvOnyA1/74LLCAv/nuGV45nlj0R2lZcPXGeu79SD+e7y/a78FnxnnuSLzqflt76viVD68HWDBuWfDUgRme3D+9ID6vjfW2RfnlD/bhvK5algX7jyf5/r5piq43//1Q0ObuW7vp76x506g8tndqPlqObXHHOzu5emN91f0s69x52ReOxvn+y9OUVS0jKVqG2b0hxife18OGzlo8D/7q0VMMjWYA2NYb4xc/0Mv7rm1lY1cN9z92hiNn0kBlpfTLH1pPX3sU3/f5+x+McOB4ZTXW0xblMx/t58btTfzup3fyD0+N8tLAHACdTWE+e0c/fe01APzznnH2HJoFoKUhzOfv6Gfn+hi/++md/MsLE/xwfyUiDbUBPn/nBgqux3/5myMLTrDWRQN87mMb6O+s4Xd+aTtPHZjh4ecmFjzPkek89z18ErfkcSGTc8VF35tNufz5P58gnSvNf8+2LH7pg31s7al7S79rWZ0ULcO01oe5ZXcLlmXhljwOnEiy91glPiXPxwf62mtY1xrloWfPhaChNsi7dzUTDNj4vs+R0yl+dLASnx19dXieT1dLhM7mMD86ODO/X03E4eadzdRFK/+pHB/Lzu/X0xbBLa+ntSHMLVeFOXgyOb9fOGhz47Ymjo9l+JNXZ/DOa09TXZBPfbCPda1R3r27hZHp/KLnmc6V2HNolnzxwtE6nw+kciUm4nn2HJolkTkvWjbc+a7OJT2OrH6KllwRSmWf//nAII5tkcqW3nwHMZaiJcuiWPI5dCrF6EwefGhvDNPVXDlJHqsJEArYpHMlhkYzjMzkFu1fE3a4akP9gvNdr8m7HoMjacqvG5pJLj5c7GwO09UcIVYTpFjyGBxOMzSaWdL1QLI6KVqyLObSLv/160fwffB8+MD1bXz67Il8Cwg4FvuPJ/mt+14l75YX7d/THuX3P7Or6mOfHM/y61/eTzq3eL/X+8iNHXzy/b0EbIuZZJH/dv9RJuKFRS8aiDkULVk2pfK5Mti2RSiw8LLA1oYQd727i8OnUvOXaLgln++8MElLbPElDwHH5r3XttJSH+KnbulmYDjNcxe4RKO7JcItu5vZvaGegGPx1P4ZBkczJLMuZRXLaIqWXDavPyRb1xrlX9+5gQeeGJmPVsH1+LvvD1fdPxKy2dUfY3tfjM9+tJ/vvjh5wWht7Krh1+7aiG1blMs+D+4Z5/kj1bcVsyhasizqawL8yofXMzlX4G+/P8xTB2YYm6m8Slgbcfj0h9fTHAut8CzFRHobjyyLSMjm1qtauHZzA5ZVOQ/1+N4pHt87xZP7Z8gVLnw+qjbiUBN23tbPtSyoizhEww5YUCiWSeVcyq8/ay/G0kpLVpWasMMX7tmGW/L4H/9voOqrh2+krSHMf/6FbXQ2RypX+e8Z55+eGWMiXlieCctlp2gZJpUtcfh0mo6mMA21Afraa8jmK6uW3rYoFjCdKDARLyy4KjybL3PkTJr2xjDtjSHWtUbY3lu5Qry/swbbsoiniozHCyTS7vx+BddjYDg9f+lAR1N4fr+OpvDZ66JchqfzCy45cEs+g6MZ5tIu23tjeOed/I7VBAgGbLL5Eqcnc0zMLQxKoejhOBbbeuoWXBEfDtlEQg6FYplTk7nK5RSvE3Asetuj84ee8bTLqYnFl1SIuaylflir7qe1Otg2hAI2n/loPx9/TzfFkj8fBMe2CAYs/u/jw3z9sdMUXW/+pX3bqryv71+9p5vPfrQft+TPv4r22mM+9OwE/+vbQwvGLKsy9oHr2/mtn91MqezPvypoWZUr358+OMvv3H+E4nn7QWXs+q2N/Kd7ti1476F1di4HTyT57b84RK5YXvBKYyhos3VdLV+8dyeRoLNwv4DN6ckc//YrB0ikXdzywv98u1sifOU3rpmP1n0Pn+T+x85cql+/LLOl5EgrLcN4HuSLHvuHEoSD1U9JHjyZXPT2F8+v7HfkdJoH94xX3W/fYGLRfr5fWW0dH8vw4DPjlYusXufYcJpclbfbFFyP0Zk8j700iV3lNg/DUzmyhdKii0SLrsdUosj39k4RDCx+jtOJIqlsaVGwoLKifPSFSWoizvzc5MqilZaIrBq6c6mIXHEULRExiqIlIkZRtETEKIqWiBhF0RIRoyhaImIURUtEjKJoiYhRFC0RMYqiJSJGUbRExCi6y4PIZVbbVUtdd/VPu86MZ0iP6M4Ub0TRErnMum7qYtOdm6qOnfjOCQa+OXCZZ2QWRUtkGdRvraduffXVVHhdmEwqU3Us1BWi+wPdVccyZzIkjiQu2RxNpWiJvF1vcIu52KYY7e9qv+B4Jlk9WsG2IB1tHVXHpp6bInH0DaK1Rj7OUTcBFHmbanfXEu4LVx2LtEYINV7aj0grJorkpxbfFx+gMFwgs796CE2i2y2LXEIBy6EhVI91dolV31RPtDN64R0u8fn0WicKnQ1Vx/LZHInwawH1mSumKPmlqtuaTistkSXqr+vhC9f8GyJOJQ522MYKrI6/C7/k4xUqN9svei5ffOXLDKZOruyk3gattEQu0oa6XprDjQB013TQFmkh7Jx32Hfhz5y9vCwgUvlX13O5qmkbDaEYAPFiguOp0ys3t0tMKy2RN/Cbuz/L+zpvrnxhWdhYq/5vwfd9PPzKRykBP5p8gd/b/2crPKul0UpL5C1qDTfzvu534ViVjyDbGOvDsZ032Wt1sSwLB2v+1c31dT18YuNd+IDnezwxtoeJ/PSKzvFiKFoi52mPtnDPxp8k5Cx+5c/3ffzzriuwzlt1reaxvtp1/MKmn8ayLEpeiSOJQUVLZC0Yy03y18e+RdFzAbij9328o/VqAKYLcf5y4BvkygUAbl93K+9qvx6AuWKSrx37Bmk3C8B7u97JbZ3vBCBdyvC1gW8wV0wB8O6O6/lA960A5Mp5vjbwDWYKcwDc2HoNH+l9L1A52f5Xxx5gPFeJz4817+CuvtsrYfJLfH3wHziTqXwo787GLXy8/yPL+ru5nBQtkQvwfZ9sOUexXInUVH6Wo4khCl4RgOtadhMvVC72nM7PcjR5nGwpB8Cuxi3zYzOFOAOJ4yTdyjUQm+vXz48liikGkieYPRumvtru+bFMKctg8uT8qqgj2jo/li8XOJY8yUi2EqbGUIy5YhIA1ysxmDzFifQZgPkT8lcKnYgXOc/Oxi38/vX/jpATwvd9/mLg73hm8iUA1tV08pltP0/Yrhw6fvv0ozw/tQ+A9kgLn9t+D1Gn8hLeI8M/4KmJ5wFoDjXyue33EAvWAvD46I/43tjTANQH6/j89ntoDFWuv/rh+HP8y8gTANQGonxu2z20RpoB2DO1lwdPPwZA2Anxq9s+SVe0ctX93pmDfPPkwwAE7QD3bv05+mrXARBxwjSG6ucPD//j3j/gldnDy/MLvEg6ES9ykeqCtfPRaIs00xVtn7/koT3SMj/WGmmmM9pGTSC6aKw51EhntI36UOW9iO3Rc2P1wTo6o+00hSvR6oi2zo/VBqJ01LTRHmmpjEXOjUXsEJ3RNrpqKtHqyJ4bC1qBBWNXGq20RM7z+pVW2ffwqFy0aWERsJz5v4WyV6b8dsb8MmV/KWMQsALnjXmU/XLVMc/3KJ035lgB7Cp/s1ppiVxhZgpx/vH0YwQsG7DYFd5IZ6B1pad1USbLsxzIDwI+ZTym8rMrPaWLopWWyBv4VOOd3BjdvdLTuCh7c4f56ty3V3oaS6KVlshFeqG+nuEms88NTcyNwtxKz+LSUbRE3sCZoMVU+OxHKdg2TiDKG95Iawm8chG/XKw6ZjlBbKf67W6Wzscr5fC9yrmxfPDKOkrS4aHIG7BDjVhO5RXBSEMvPTf+Onbg4qIyc+wRZga/U3WseeP7ad1210U9vl92GX7+T8nNnTj7dR6vGL+ox7xcdHgocpG84hyvHVuVI1GcgI0TvMg/m3Kacm6s6pBfThO4yMf3rDJeceaCP8N0+ggxETGKVloiS1QuJImf+B6WHQSgsedqapv733y/Ypapoacou5X3JebiQxfcNj93gpnBRwGwAyHaNt1CIFz9AzLOl42fIX7mZQB8v4Sbn3vTfUylaIkskZudZuzlr85/HWv6TWKbdr3pfrlEgon9X6eYffPro9Lj+0iP7wMgGGmgZ8cN1DRV/6CLBfuNnmD0pa+86XZXAh0eiohRtNISeZu8kkupWLndjGXZ2IHwubfVlIp4XuWDJcpufsH9rZbKx6dcys//DNsOYAcq73v0fR+vVMA/+5Yf7wKXUFyJdMmDyNsUjrUTijYCEGnoYsftv00gXLmTw+mXHmDi8HcB8Mou2fhpfO+t3VDesmxqmvuwX3uD9tb3sv7GTwJQdnMc/u6XyMWHAXDzSfLJ8UvxtFaULnkQWUaF1CSF1CQAbiFFevr4fLTSU4OkJi/u4+193yMzc3L+62hTL+np40AlWqnJY+TiZy7qZ5hIKy2RS8GycII15+7IUCrgn7154CX7EXYQJ1i5sNX3fcpuDs4eHl4plpIjRUtEVo2l5EivHoqIURQtETGKoiUiRlG0RMQoipaIGEXREhGjKFoiYhRFS0SMomiJiFEULRExiqIlIkZRtETEKIqWiBhF0RIRoyhaImIURUtEjKJoiYhRFC0RMYqiJSJGUbRExCiKlogYRdESEaMoWiJiFEVLRIyiaImIURQtETGKoiUiRlG0RMQoipaIGEXREhGjKFoiYpTASk9ARFbejZvruWlrAwDZQplvPjNBMldetF04aPPxm9tpqgsu+P739s9yaDhzWeaqaIkIm7qifPD6FgDiaZeHXpqej1YgYBEIWADEog637G6kpyWyYP+hqRzHp3NVH9t1fcpl/5LNVdESEewum+C1lRwE4z5W6NzYbbc1c/fdHZXtLIt1LWGCgYVnln5+Yzd35NqrPvYDD4zzxBOzl2yuipaIkMiWOHV2pZRM+vihDgI1HgChuhYi9V3z204VgeLrHiAIkbNHjL7nUi5MgV/Z37Iu7Vwt3/eXtG6zLvVPFpFVIxi0CIUqqycrECO68TewQi3zY+HQ0l+zK+VGmX31j/HLeQAKBY9SaWmHh0vJkVZaIoLr+rhumXXbGqlvbaKuL04um2X4yCy1nbW0rYtV3S8+mWXieBIA27Ho29VMsC1FjdVMYiLJ2FACqKy2enc24wRtTh+cobzEiFWjaIkIUAnLdbf30X91C3CAkaNxDj20jw3re7jxPVuq7nPwhyMMPHoUgGDYYcfHr6OtLwZs5sie8flo2Y7NjR/rJ1oX4puDCcpp923PU9ESEVp3t9J2TRvl3nqmnEoW5uzKIWHWspmyHUaeHiEzlmH97esJN4QBCGxtZvsntjP27BipU0n2Pnqaxt46+j+wHmtTE9s/sR0ACxjYN0MxUcAtLL6U4q1QtESEhk0N9L2/j1LZZ6bsYwds0raDFbQpWBYzZRjYP8PM4RkiN6+jLhbBciycvgbW99aTGc2QOp1i4PkJ6kYy1L6nj2BHHf3r6gEou2Ve+sOXiA/G8S/i0BB0Il5EgE13bWLzXZsZ+OYA6bE0uz61CyfkkB5JM3t0lqlXpsiMZShlS8R6Y9T317Pjnh04QQeAzESG3FSOQ/cfIh/PE+uN0XZNG5s+tgnLsvA9n/RImvRImlf/5lVKuVLVeehEvIi8JW7WxU254IEdsAnXh7Esi2KiiOd6+J5P8lQSy7bAO7dfIBIgVB/CciqBKiaL82EqJou4ORc7ZBOMBS/6zYOKlohUWLDtZ7fhez7BmiBzg3Ps+7N9dN/czU1fuIlX/+pVJl+eXLSb7/scf+g4o3tGKWVL1HTWcP1vXE8oVrlC9eSjJznzwzPz25ay1VdZS6VoiQiZ0QyTeydp3NRIuLFykt0rexRTRXzfJxQL0bS1Cd/zmT167ur27FSW5KkkqeEU7tlXBC3bIlgXxM26TL86TfJMcn7sUtBdHkSE8RfG2fflfSROJqqOW5ZF/4f6uereq4g0nXvf4fTBafZ9eR+zhxe/TWf2yCz7vryP6f3Tl3SuWmmJCAC+5zP69Chzg3MA5KZz+J5PfCDOwLcGACgXyxRTlfNVx/7xGOnhNLzu3HkxUWTon4bITGQWjV0KevVQRFaNpeRIh4ciYhRFS0SMomiJiFEULRExiqIlIkZRtETEKIqWiBhF0RIRoyhaImIURUtEjKJoiYhRFC0RMYqiJSJGUbRExCiKlogYRdESEaMoWiJiFEVLRIyiaImIURQtETGKoiUiRlG0RMQoipaIGEXREhGjKFoiYhRFS0SMomiJiFEULRExSmClJ3C5hQIWH3tHGy2xIABHRjI8eWiu6rabOqK876pmLGvx2KmpHI/um13GmYpINWsuWgHH4sd3N7GhPQpAJGQviFbAsXitUX1tEe68oRXLsvB9n5Lng1/Z5sWhJE8cjOP5i3+G71PZVkQuuTUXrTcSdCx+9fYe+lrDANTXnPv1lD342uOjxDMuv/bhXravq+W///ymqo8zOJ7jq4+PVA2aiFycNRct34epZJFwoHI6z/V9ursrkQoFbLZvrqH/7CrsfFYZ0laZFGWIQKwmwM7murMPCrOzLvm8B0BdpFg5pFS0RC45y/f9Jf1pWdVO7BiqNuLgnH06t97WzKd+ZR0AlgW1YQfHrv5cM4Uyng91YWfBeS7Pg9/7/SFe3psEKoeG2YK3rM9B5Eq0lBytuZUWQCZfxnKihOq3MuM2cWysnVL6NOXCLKGGLViB2gXblzLDlPNTBOs3Y9khiomj4JcACES7sCOdzJQi5MMJismj4BUBcCIdBGrW4aaP4xXnLvfTFLkircloATiRNhq2foaT+Qh/+g1IDN1PfuokTbs/Tqhuw4JtkycfIDv2PM27fhI73MLs/ifw3AQAdb27qOv9IL4DDZvnmDnwRbxC5VXFSMu11K2/m7mjf05h5qXL/hxFrkRrLlpOwGb3bd3UtbYQbXsVy6r8Cg4lphmb9ti07RjBmmkO/nCUxo4oW27ooLjJw033E20dwQrMkuvvZnYkypE94+TnDuH7Jbbf3ElDm01+QxdeqRGAqbEZxk59m1J2ZAWfsciVZe1FK2ix89ZuWtfV4nuHwbKwbIuxA3NMBnzWbxoiGHZ48YG9hFqa2bZrJ5ZtYdl9+OUxwMeyOzmxP8jACxO4qWOUUsfo6ria9bubsex2XjvhtffR0ww9/yK+XkYUuWTW3In4QDTAbf/+BgJBh6N/f4Smbc1s+FA/s2MZUtN54s+Okp3MMHkiRaQuSMu6Wnrf20v7tR0MfOsoxWSRHZ/YQcn3mTqTZuRHI4ztGaOtr476zlq2f2I74frKq5HJmTxzE1mGHhwiPhBf4WcusvrpRPwF5C0byj5TEznoKtJuWTiNEWocm5OpIqnpPJ7vk00WySaLhDY3E+gvMDWRw027pH0IRILUdtUSioUAmJstULBtej2LYtmnmCqCbVHbVUsguiZ/zSLLYk2utG76DzdR21VLMVXECTkEa4McfeAo4y+Os+sXd2EHbPb+yV5KudL8PsHaIDs+uYOGjQ2EYiFmDs1w8C8P4mZdyoUyV33mKlp3thKKhZgbmuOV//MKfrnyq3UzLp6rSyBE3oxWWlX4ZZ/4sTilfInGjY1YZy/YcrMuhbkCgZoA4YYwLbtayE5kSZ1JUcqVKLtlAtEAgWiA2aOzzA3OkY/nKxeQWhCqCxGoDRA/FmduaI5CvKBzWSLLYM3d5aFcLHPo/kMc+dsjlN1y1W0izRGu+fw1bPjIhkVjhUSB/fftZ/CfBhdd8e6mXQ587QAD3xpQsESWyZpbaQHgQz6e58QjJ+ZXWsmTSfyyz8iTI0wfmAYgPZw+t4vnM/L0CMGaIKVsaWGwfBh7dozZw7O4aVdv3xFZRmvunJaIrF5LydGaOzwUEbMpWiJiFEVLRIyiaImIURQtETGKoiUiRlG0RMQoipaIGEXREhGjKFoiYhRFS0SMomiJiFEULRExiqIlIkZRtETEKIqWiBhF0RIRoyhaImIURUtEjKJoiYhRFC0RMYqiJSJGUbRExCiKlogYRdESEaMoWiJiFEVLRIyiaImIUQIrPQGRlbK7r5brNtQDkHc9vvPyNKlcedF2QcfiQ9e20FQbXPD9ZwcSDIxlL8tc5RxFS9asHT21/MwtHQAksiWeOhyfj5ZlgX32OCQStvngtS30t0cX7D+dLjI0WT1ange+v3xzX8sULVmznHab4NUOAMGkjxW25sduvrmRO+9sr2xnW/R01RAMLTyb8tPdnbwn1VL1sR9+eIqnnoov08zXNkVL1qys5zFdcgFIlT38YD12qARAQ0sr/Zu757fNn/3nfHUdMeo6zn7hlym7KfA9AGIx/WktF8v3l7aItSzrzTcSMUg0alNTU1lp4dRid38eApWVUyRqU1frLPmxSvlJ5g7/b7xS5XAxkymTz3uXfM5XuqXkSP87kDUrl/PI5Txae+uoa64l1l/ELaWYPJHEioYJ19dW3S8dLzAzkgbAsi06+uuJNGSp7aglPVtmergyhgUd62M4IYfx4wm8kk5yXQqKlqx5N3y0n03XtoG1j7HBBMce3UfPrd3c8v4tVbc//PQYg48dASAQtvmJj11Lx4YY+Ns49sIEj371EAC2bfGuuzcRa47wrS/tJZssXrbndCVTtGTNatzSSMuOFuipJx6oHAqmHPB9j4LlE3csJl+eJDOeYd0t6wjFQgDY/TE23bmBqVemSA+nOPz0CONnYvTcsg6rr4FNd26a/xmnB5N42RncwuJLKeTtUbRkzWre1szmn9qMB0z7gAUJ2wEbcpbFtOVw8KUppvZNcePVHcTqo2CBs6GJLRuaKKaKJE8nOfjkKLVdtQRvWkewt54tfQ0AeK7Hi3/4IrNHZ0FHhpeMTsTLmrXxjo1suXsLJx4+QXo0zdaf2YodsJkbnCM9nGZuaI7kySSFRIGmrU3Ur69ny91bcEKVVVnydJLMeIZj/3CMwlyBpm1NtOxoof9D/ViWhe/5lccaTXP0gaOUsqUVfsarn07EiyxBZjxD4kQCz/UI1YVo2NhAdiLL3NAcpVwJ3/OZPTI7/++vqWmrIVgTxAk7eK5H4kSCSHMEgFK+RLlQpqajBjtoYzt6x9ylomjJmrf141vxXI9wY5jUmRT779tP29VtvPML7+TI3x9hcu9k1f2OP3KcsT1jFOYKRFojXPtr1xJuDANw6rFTDD85DIBf9ilmdBL+UlG0ZM3KzeSID8Sp664j2lp5i47neuSmc3hlj2hblIb+BorJIqnTqfn9CokCmfEMqTMpctM5AGzHJtoaxSt7xAfilbGp3Io8ryudoiVr1thzY0y8OME1n7+Gjus6qm6z4aMb6PnxHl740gvz35s+MM3Bvz6IX158/mX2yCwH7juAV9aFpctF0ZK1ywPP85jcO0n27Bufc9M5/LJP6lSKk4+erGzmerhpF2w4/fhpkqeT+K+7UNTNuJz+3mky4xm8koK1nPTqoYisGkvJkV7SEBGjKFoiYhRFS0SMomiJiFEULRExiqIlIkZRtETEKIqWiBhF0RIRoyhaImIURUtEjKJoiYhRFC0RMYqiJSJGUbRExCiKlogYRdESEaMoWiJiFEVLRIyiaImIURQtETGKoiUiRlG0RMQoipaIGEXREhGjKFoiYhRFS0SMomiJiFEULRExiqIlIkZRtETEKIqWiBhF0RIRoyhaImIURUtEjKJoiYhRFC0RMYqiJSJGUbRExCiKlogYRdESEaMoWiJiFEVLRIyiaImIURQtETGKoiUiRlG0RMQoipaIGEXREhGjKFoiYhRFS0SMomiJiFEULRExiqIlIkZRtETEKIqWiBhF0RIRoyhaImIURUtEjKJoiYhRFC0RMYqiJSJGUbRExCiKlogYRdESEaMoWiJiFEVLRIyiaImIURQtETGKoiUiRlG0RMQoipaIGEXREhGjKFoiYhRFS0SMomiJiFEULRExiqIlIkZRtETEKIqWiBhF0RIRoyhaImIURUtEjKJoiYhRFC0RMYqiJSJGUbRExCiKlogYRdESEaMoWiJiFEVLRIyiaImIURQtETGKoiUiRlG0RMQoipaIGEXREhGjKFoiYhRFS0SMomiJiFEULRExiqIlIkZRtETEKIqWiBhF0RIRoyhaImIURUtEjKJoiYhRFC0RMYqiJSJGUbRExCiKlogYRdESEaMoWiJiFEVLRIyiaImIUQIrPYHqbFrb300o1AJAPj/G7PRzKzwnEVkNVmW0LMuhr/+TNDRdA8D05FPMTj8P+Cs7MRFZcZbv+0sqgWVZyz0XANo7309TyzvIpk9SKmcBCAZi1NT2MTX5Q2amnrks8xCRy28pOVo157Qsy8FxojQ2/Rhd6z5CMnmYseEHGRt+kGz2NF09H6O+YReOE2UVTVtELrNVc3jY1HIDG7d8jkik/YLbdPfcSUvrzQwc+WOScwcu4+xEZLVY8WiFQiF6enpo79zGpq07AfB9l+TGXlLJyuFhY/M6urscoAPfb4XyVmam05w5c4ZSqbSCsxeRy23Fz2lt2LCB++67j4aGZmwncva7PuVSDt8vA2DbAWwnet5YnomJUe69914mJyeXZV4icvktJUcrvtJyHIdYLEZdXfS871pA7QX2sIAaMpk6bFvntkTWGv3Vi4hRVnylNTU1xR/90R+xbVsD7353AwCe5/P443EmJ4sA9PSEue22JiwLfB9+8IM4g4NxksnkSk5dRFbAikcrkUjwwAMPcNNN9axf343jWHiez0MPnWJgIAfAddfVsX17L74PruvzyCNnOHgws8IzF5GVsGoODw8dyvClL53ixRcvvHp68sk5/uAPTnH8eO4yzkxEVpMVX2m9xnV9EokSxaKPZVk0Nwfp6KhcztDQEAQgn/dIJEqUSno7j8hateKXPLzmHe+I8alPdRGJ2ASDFrmcNx+nQMAiGrUpFn1yOY+vfGWYw4ezyzofEbn8jLjk4TWpVJmjR7O8WRs9DzIZ7/JMSkRWnVWz0qr8jKVtt7QZi4hpLulKa+NPv/+iJiMiciksOVr9d7xnOechIrIkq+aSBxGRpVjySis/m1jOeYiILMmST8SHm+qXey4issYV4m/+1rxV9eqhiKxtRt1uWURkKRQtETHKqrkiXuSivd0zGLpY2SiKllwRLMdi809tpq677q3t6MPxR46TGNKr46ZQtOTKYEHTliaatzW/pd18z2f0mVESKFqm0DktETGKoiUiRlG0RMQoipaIGEXREhGjKFoiYhRFS0SMomiJiFEULRExiqIlIkZRtETEKHrvoVxRlnhPSzGY7lwqVwYL2q5uI9wQfku7+fjMvDpDfia/TBOTt2IpOVK0RGTV0O2WReSKo2iJiFEULRExiqIlIkZRtETEKIqWiBhF0RIRoyhaImIURUtEjKJoiYhRFC0RMYqiJSJGUbRExCiKlogYRdESEaMoWiJilCXfblm3sRWR1UArLRExiqIlIkZRtETEKIqWiBhF0RIRoyhaImIURUtEjKJoiYhRFC0RMcr/B5wMnL5iiwDqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over..Your agent sucks!\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have your policy network and environment initialized\n",
    "display_environment(env, policy_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
