{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Definition (Atari Assault)\n",
    "\n",
    "The agent has to decide to choose an action in a space of 7 discrete space, which gives 18 possible actions. The observation state is RGB images of shape = (250, 160, 3) with integer values between [0-255]. \n",
    "\n",
    "You can learn more about it here:\n",
    "https://gymnasium.farama.org/environments/atari/assault/\n",
    "\n",
    "In this Notebook, we are going to experiment with two version of Q learning: (A) Deep Q Network and (B) PPO. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#This is the environment or game to be used with rgb output (from render)\n",
    "env = gym.make(\"ALE/Assault-v5\", render_mode='rgb_array')\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "total_n_episodes = 1000\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track episode duration when training / evaluating \n",
    "#Note: The total reward in this env represent the duration of an episode, which sometimes can be different.\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration / Total Reward')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    #The state \n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3, stride=3)\n",
    "        self.cnn2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=3)\n",
    "        self.pool = nn.MaxPool2d(3, stride=2)\n",
    "        self.cnn3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(4480, 256)\n",
    "        self.fc2 = nn.Linear(256, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.cnn1(x))\n",
    "        x = F.relu(self.cnn2(x))\n",
    "        x = F.relu(self.cnn3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the frames by switching axes and normalizing values to floats\n",
    "#Input frames should be passed in a batch mode (batch, 3, h, w)\n",
    "def prepare_frame(state):\n",
    "    \n",
    "    #Create a float tensor\n",
    "    state = torch.tensor(np.numpy([state]), dtype=torch.float32)\n",
    "    #Permute shape from (batch, h, w, 3) to (batch, 3, h, w)\n",
    "    state = torch.permute(state, (0, 3, 1, 2))\n",
    "    #Normalize values to obtain floats\n",
    "    state = state / 255.\n",
    "\n",
    "    return state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "#Track the duration of the epipsodes\n",
    "episode_durations = []\n",
    "\n",
    "policy_net = DQN(n_actions).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = policy_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to render and display the environment\n",
    "def display_environment(env, policy_net):\n",
    "    state, _ = env.reset()\n",
    "    plt.figure()\n",
    "\n",
    "    img = plt.imshow(env.render())  # Initialize the rendering with an empty frame\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    for t in range(1000):  # Run for a set number of time steps\n",
    "\n",
    "        state = prepare_frame(state)\n",
    "        action = policy_net(state).argmax().item()  # Select action\n",
    "        state, reward, terminated, truncated, _ = env.step(action)  # Take a step\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        \n",
    "        frame = env.render()  # Render the environment to an RGB array\n",
    "        img.set_data(frame)  # Update the rendering\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(frame)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        time.sleep(0.01)  # Pause to slow down the rendering\n",
    "\n",
    "        if done:\n",
    "            print(\"Game over..Your agent sucks!\")\n",
    "            break\n",
    "    \n",
    "    if not done:\n",
    "        print(\"Game over..Your agent is great!\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game over..Your agent sucks!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Assuming you have your policy network and environment initialized\n",
    "display_environment(env, policy_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fares\\AppData\\Local\\Temp\\ipykernel_10620\\273173432.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  state = torch.tensor([state], dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 2000\n",
    "else:\n",
    "    num_episodes = 100\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = prepare_frame(state)\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = prepare_frame(observation)\n",
    "            \n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            if(t % 10 == 0):\n",
    "                plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your policy network and environment initialized\n",
    "display_environment(env, policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
